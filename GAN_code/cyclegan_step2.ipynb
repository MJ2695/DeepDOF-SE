{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import resnet_network\n",
    "import image_preprocess\n",
    "import network_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 2\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "EPOCHS = 5\n",
    "\n",
    "STEPS_TO_FILL_POOL = 10\n",
    "FAKE_POOL_SIZE = BATCH_SIZE*STEPS_TO_FILL_POOL\n",
    "FAKE_POOL_PROBABILITY = 0.8\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "load_checkpoint_path = 'directory_for_loading_checkpoints'\n",
    "checkpoint_path = f'directory_for_saving_checkpoints'\n",
    "log_dir = 'directory_for_saving_logs'\n",
    "PATH = pathlib.Path(r'data_directory')\n",
    "summary_writer = tf.summary.create_file_writer(log_dir+datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from folder and setup dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2\n",
    "\n",
    "train_x_files = tf.data.Dataset.list_files(str(PATH/'trainA/*.png'), shuffle=False)\n",
    "train_y_files = tf.data.Dataset.list_files(str(PATH/'trainB/*.png'), shuffle=False)\n",
    "test_x_files = tf.data.Dataset.list_files(str(PATH/'testA/*.png'), shuffle=False)\n",
    "test_y_files = tf.data.Dataset.list_files(str(PATH/'testB/*.png'), shuffle=False)\n",
    "\n",
    "train_x = train_x_files.cache().map(image_preprocess.load_image, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE, seed=SEED).batch(BATCH_SIZE)\n",
    "train_y = train_y_files.cache().map(image_preprocess.load_image, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE, seed=SEED).batch(BATCH_SIZE)\n",
    "test_x = test_x_files.cache().map(image_preprocess.load_image, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE, seed=SEED).batch(BATCH_SIZE)\n",
    "test_y = test_y_files.cache().map(image_preprocess.load_image, num_parallel_calls=AUTOTUNE).shuffle(\n",
    "    BUFFER_SIZE, seed=SEED).batch(BATCH_SIZE)\n",
    "\n",
    "print(f'Number of training images: {len(train_x)*BATCH_SIZE}')\n",
    "print(f'Number of test images: {len(test_x)*BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CycleGAN class and create a cyclegan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        super(CycleGAN, self).__init__()\n",
    "        \n",
    "        self.generator_g = resnet_network.build_generator_resnet_9blocks(skip=False)\n",
    "        self.generator_f = resnet_network.build_generator_resnet_9blocks(skip=False)\n",
    "\n",
    "        self.discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "        self.discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "\n",
    "        self.generator_g_optimizer = tf.keras.optimizers.Adam(2e-04, beta_1=0.5)\n",
    "        self.generator_f_optimizer = tf.keras.optimizers.Adam(2e-04, beta_1=0.5)\n",
    "\n",
    "        self.discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-04, beta_1=0.5)\n",
    "        self.discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-04, beta_1=0.5)\n",
    "\n",
    "    def call(self, real_x, real_y, fake_pool_x_batch=None, fake_pool_y_batch=None, training=False, fake_pool=False):\n",
    "        fake_y = self.generator_g(real_x, training=training)\n",
    "        cycled_x = self.generator_f(fake_y, training=training)\n",
    "\n",
    "        fake_x = self.generator_f(real_y, training=training)\n",
    "        cycled_y = self.generator_g(fake_x, training=training)\n",
    "\n",
    "        disc_real_x = self.discriminator_x(real_x, training=training)\n",
    "        disc_real_y = self.discriminator_y(real_y, training=training)\n",
    "\n",
    "        if fake_pool: \n",
    "            disc_fake_x = self.discriminator_x(fake_pool_x_batch, training=training)\n",
    "            disc_fake_y = self.discriminator_y(fake_pool_y_batch, training=training)\n",
    "        else:\n",
    "            disc_fake_x = self.discriminator_x(fake_x, training=training)\n",
    "            disc_fake_y = self.discriminator_y(fake_y, training=training)\n",
    "\n",
    "        return fake_x, fake_y, cycled_x, cycled_y, disc_real_x, disc_real_y, disc_fake_x, disc_fake_y\n",
    "\n",
    "        \n",
    "    def losses(self, real_x, real_y, fake_pool_x_batch, fake_pool_y_batch, fake_pool=False):\n",
    "\n",
    "        __, __, cycled_x, cycled_y, disc_real_x, disc_real_y, disc_fake_x, disc_fake_y = self.call(real_x, real_y,\n",
    "                                                                                                       fake_pool_x_batch,\n",
    "                                                                                                       fake_pool_y_batch,\n",
    "                                                                                                       fake_pool=fake_pool)\n",
    "        gen_g_adver_loss = network_losses.generator_loss(disc_fake_y)\n",
    "        gen_f_adver_loss = network_losses.generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = network_losses.calc_cycle_loss(real_x, cycled_x) + network_losses.calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        gen_g_total_loss = gen_g_adver_loss + total_cycle_loss\n",
    "        gen_f_total_loss = gen_f_adver_loss + total_cycle_loss\n",
    "\n",
    "        disc_x_loss = network_losses.discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = network_losses.discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "        return gen_g_total_loss, gen_f_total_loss, disc_x_loss, disc_y_loss\n",
    "\n",
    "    def generate_images(self, real_x, real_y, training=False):\n",
    "        fake_y = self.generator_g(real_x, training=training)\n",
    "        fake_x = self.generator_f(real_y, training=training)\n",
    "\n",
    "        return fake_x, fake_y\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, real_x, real_y, step, fake_pool_x_batch=None, fake_pool_y_batch=None, fake_pool=False):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            gen_g_total_loss, gen_f_total_loss, disc_x_loss, disc_y_loss = self.losses(real_x, real_y,\n",
    "                                                                                        fake_pool_x_batch, fake_pool_y_batch,\n",
    "                                                                                        fake_pool=fake_pool)\n",
    "        \n",
    "        fake_x, fake_y = self.generate_images(real_x, real_y, training=True)\n",
    "\n",
    "        generator_g_gradient = tape.gradient(gen_g_total_loss, self.generator_g.trainable_variables)\n",
    "        generator_f_gradient = tape.gradient(gen_f_total_loss, self.generator_f.trainable_variables)\n",
    "\n",
    "        discriminator_x_gradient = tape.gradient(disc_x_loss, self.discriminator_x.trainable_variables)\n",
    "        discriminator_y_gradient = tape.gradient(disc_y_loss, self.discriminator_y.trainable_variables)\n",
    "\n",
    "        self.generator_g_optimizer.apply_gradients(zip(generator_g_gradient, self.generator_g.trainable_variables))\n",
    "        self.generator_f_optimizer.apply_gradients(zip(generator_f_gradient, self.generator_f.trainable_variables))\n",
    "\n",
    "        self.discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradient, self.discriminator_x.trainable_variables))\n",
    "        self.discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradient, self.discriminator_y.trainable_variables))\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('X TO Y generator: Total loss', gen_g_total_loss, step=step)\n",
    "            tf.summary.scalar('Y TO X generator: Total loss', gen_f_total_loss, step=step)\n",
    "            tf.summary.scalar('X TO Y: Discriminator loss', disc_x_loss, step=step)\n",
    "            tf.summary.scalar('Y TO X: Discriminator loss', disc_y_loss, step=step)\n",
    "        \n",
    "        return fake_x, fake_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_pool_x = []\n",
    "fake_pool_y = []\n",
    "\n",
    "cyclegan = CycleGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load current model\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_f = cyclegan.generator_f,\n",
    "                           generator_g = cyclegan.generator_g,\n",
    "                           discriminator_x = cyclegan.discriminator_x,\n",
    "                           discriminator_y = cyclegan.discriminator_y,\n",
    "                           generator_f_optimizer = cyclegan.generator_f_optimizer,\n",
    "                           generator_g_optimizer = cyclegan.generator_g_optimizer,\n",
    "                           discriminator_x_optimizer = cyclegan.discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer = cyclegan.discriminator_y_optimizer)\n",
    "\n",
    "# load previous checkpoint\n",
    "load_ckpt_manager = tf.train.CheckpointManager(ckpt, load_checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if load_ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(load_ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored')\n",
    "else:\n",
    "    print('Train from scratch')\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    for n, (image_x, image_y) in tf.data.Dataset.zip((train_x, train_y)).enumerate():\n",
    "        if n < STEPS_TO_FILL_POOL and epoch == 0:\n",
    "            fake_x, fake_y = cyclegan.train_step(image_x, image_y, n+epoch*len(train_x),\n",
    "                                        fake_pool=False)\n",
    "\n",
    "            fake_pool_x.append(fake_x)\n",
    "            fake_pool_y.append(fake_y)\n",
    "        else:\n",
    "            p = tf.random.uniform(shape=[])\n",
    "            if p>FAKE_POOL_PROBABILITY:\n",
    "                __, __ = cyclegan.train_step(image_x, image_y, n+epoch*len(train_x),\n",
    "                                    fake_pool=False)\n",
    "            else:\n",
    "                rd_ind_x = random.randint(0, STEPS_TO_FILL_POOL-1)\n",
    "                rd_ind_y = random.randint(0, STEPS_TO_FILL_POOL-1)\n",
    "                fake_pool_x_batch = fake_pool_x[rd_ind_x]\n",
    "                fake_pool_y_batch = fake_pool_y[rd_ind_y]\n",
    "\n",
    "                fake_x, fake_y = cyclegan.train_step(image_x, image_y, n+epoch*len(train_x),\n",
    "                                            fake_pool_x_batch, fake_pool_y_batch,\n",
    "                                            fake_pool=True)\n",
    "\n",
    "                fake_pool_x[rd_ind_x] = fake_x\n",
    "                fake_pool_y[rd_ind_y] = fake_y\n",
    "\n",
    "\n",
    "        if n%10 == 0:\n",
    "            print('.', end='')\n",
    "\n",
    "        \n",
    "        if n%200 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f'current batch: {n} out of {len(train_x)} batches, epoch {epoch+1}')\n",
    "            \n",
    "            image_preprocess.generate_images(cyclegan.generator_g, image_x)\n",
    "            image_preprocess.generate_images(cyclegan.generator_f, image_y)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                            ckpt_save_path))\n",
    "\n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                    time.time()-start))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c631db9097f65b7be12866a4e1707f1d2f896104c450bb1f031ce21d398d87ed"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
